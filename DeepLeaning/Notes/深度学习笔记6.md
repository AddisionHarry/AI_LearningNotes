## 5.2 自然语言处理和词嵌入
目前为止都是使用一个 One-shot 向量$O_i$来表述一个单词，这种方式存在的问题就是这样训练出来的网络对于相关词语的泛化能力不强（主要原因就是任意两个不相等的 One-shot 向量之间的内积都是零，因此网络不能从 One-shot 向量中获得词语的相关信息）。这里的解决方案就是转而构造若干特征，使用特征向量来描述一个单词：
![使用特征向量描述词语](../Pic/image-27.png)
使用这种特征向量描述方式训练出来的网络在泛化能力上明显是要优于使用 One-shot 向量的网络的。此外，在 NLP 领域，通过这种方式构造出来的特征向量有的时候也会通过一些手段降维到可视化空间（聚类、 PCA ......），这种过程也被称为**单词嵌入(Word embedding)**。这里的单词嵌入和前面在学习卷积神经网络的时候学到的*编码*概念很相似（可以去回顾一下人脸识别任务），其实在很大程度上，这里的*嵌入*和*编码*表达的就是相同含义。


实际上的单词嵌入任务在开发的时候会结合*迁移学习*来实现，比如下面的任务结构：

1. 从大量的文本集(100万-1亿量级)中学习各种单词的特征（进行单词嵌入）；
   当然也可以直接从网上下载预训练好的嵌入网络
2. 将嵌入好的网络迁移到需要使用的任务中，此时就可以使用更小的数据集(比如10万个单词)进行训练
   好处就是可以在这个过程中获得更加紧凑的向量（最终是要将特征向量压缩成 One-shot 向量）
3. 当然也可以在新的数据上选择是不是还要对模型进行微调

单词嵌入还有一个比较好的特性就是可以实现**类比推理**，比如现在有四个单词，嵌入以后结果为
$$
\boldsymbol{e}_{1000}, \boldsymbol{e}_{1005}, \boldsymbol{e}_{1500}, \boldsymbol{e}_{1520}\\
$$
如果我们已知这四个向量满足特性的关系：
$$
\boldsymbol{e}_{1520}-\boldsymbol{e}_{1000}\approx\boldsymbol{e}_{1005}-\boldsymbol{e}_{1500}\\
$$
而且已知在很大程度上$\boldsymbol{e}_1005$和$\boldsymbol{e}_1500$对应的单词是非常相似的（比如 man 和 woman），那么我们就可以得出$\boldsymbol{e}_1000$和$\boldsymbol{e}_1520$对应的单词也非常相似的结论，这就是单词嵌入带来的类比推理功能。实际上，如果给定前三个单词，那么该算法需要实现的就是：
$$
\min_{w}\textrm{Similarity}(\boldsymbol{e}_w,\boldsymbol{e}_{1000}-\boldsymbol{e}_{1005}+\boldsymbol{e}_{1500})\\
$$
研究者的研究结果表明，在嵌入体量非常大的情况下（嵌入以后的特征向量维数很高），只要这两个向量能达到30%-75%的相似度，结果就基本可以让人满意了。


在实际使用过程中，这里的相似度函数可能会取为向量之间的夹角：
$$
\textrm{Similarity}(\boldsymbol{u},\boldsymbol{v})=\arccos\frac{\boldsymbol{u}^T\boldsymbol{v}}{||\boldsymbol{u}||_2||\boldsymbol{v}||_2}\\
$$
当然其实也可以使用向量末端点之间的距离，不过这里不再赘述了。
