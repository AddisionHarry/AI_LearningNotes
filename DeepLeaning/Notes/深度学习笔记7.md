## 5.2 自然语言处理和词嵌入
目前为止都是使用一个 One-hot 向量$O_i$来表述一个单词，这种方式存在的问题就是这样训练出来的网络对于相关词语的泛化能力不强（主要原因就是任意两个不相等的 One-hot 向量之间的内积都是零，因此网络不能从 One-hot 向量中获得词语的相关信息）。这里的解决方案就是转而构造若干特征，使用特征向量来描述一个单词：
![使用特征向量描述词语](../Pic/image-27.png)
使用这种特征向量描述方式训练出来的网络在泛化能力上明显是要优于使用 One-hot 向量的网络的。此外，在 NLP 领域，通过这种方式构造出来的特征向量有的时候也会通过一些手段降维到可视化空间（聚类、 PCA ......），这种过程也被称为**单词嵌入(Word embedding)**。这里的单词嵌入和前面在学习卷积神经网络的时候学到的*编码*概念很相似（可以去回顾一下人脸识别任务），其实在很大程度上，这里的*嵌入*和*编码*表达的就是相同含义。


实际上的单词嵌入任务在开发的时候会结合*迁移学习*来实现，比如下面的任务结构：

1. 从大量的文本集(100万-1亿量级)中学习各种单词的特征（进行单词嵌入）；
   当然也可以直接从网上下载预训练好的嵌入网络
2. 将嵌入好的网络迁移到需要使用的任务中，此时就可以使用更小的数据集(比如10万个单词)进行训练
   好处就是可以在这个过程中获得更加紧凑的向量（最终是要将特征向量压缩成 One-hot 向量）
3. 当然也可以在新的数据上选择是不是还要对模型进行微调

单词嵌入还有一个比较好的特性就是可以实现**类比推理**，比如现在有四个单词，嵌入以后结果为
$$
\boldsymbol{e}_{1000}, \boldsymbol{e}_{1005}, \boldsymbol{e}_{1500}, \boldsymbol{e}_{1520}\\
$$
如果我们已知这四个向量满足特性的关系：
$$
\boldsymbol{e}_{1520}-\boldsymbol{e}_{1000}\approx\boldsymbol{e}_{1005}-\boldsymbol{e}_{1500}\\
$$
而且已知在很大程度上$\boldsymbol{e}_{1005}$和$\boldsymbol{e}_{1500}$对应的单词是非常相似的（比如 man 和 woman），那么我们就可以得出$\boldsymbol{e}_{1000}$和$\boldsymbol{e}_{1520}$对应的单词也非常相似的结论，这就是单词嵌入带来的类比推理功能。实际上，如果给定前三个单词，那么该算法需要实现的就是：
$$
\min_{w}\textrm{Similarity}(\boldsymbol{e}_w,\boldsymbol{e}_{1000}-\boldsymbol{e}_{1005}+\boldsymbol{e}_{1500})\\
$$
研究者的研究结果表明，在嵌入体量非常大的情况下（嵌入以后的特征向量维数很高），只要这两个向量能达到30%-75%的相似度，结果就基本可以让人满意了。


在实际使用过程中，这里的相似度函数可能会取为向量之间的夹角：
$$
\textrm{Similarity}(\boldsymbol{u},\boldsymbol{v})=\arccos\frac{\boldsymbol{u}^T\boldsymbol{v}}{||\boldsymbol{u}||_2||\boldsymbol{v}||_2}\\
$$
当然其实也可以使用向量末端点之间的距离，不过这里不再赘述了。


实际上我们通过单词嵌入构造出来的是一个**嵌入矩阵(Embedding matrix)**，比如总共有 10000 个单词提取 300 个特征，最后得到的就是一个 300×10000 大小的矩阵$\boldsymbol{E}$，不难发现将嵌入矩阵和之前定义的 One-hot 向量相乘，得到的就是对应单词在嵌入以后的特征向量：
$$
\boldsymbol{e}_i=\boldsymbol{E}\boldsymbol{O}_i\\
$$
不过值得注意的就是由于矩阵的稀疏性（One-hot 向量中绝大多数元素都是 0），**在这里使用矩阵乘法的效率其实非常低**，不过在习惯上还是会使用这种数学表述来获得某一个单词的特征向量。


接下来就是训练我们需要的嵌入矩阵了。实际上这里构造学习嵌入矩阵的方式并不是使用 RNN 网络，而是使用一个监督学习网络来训练。比如有一个句子，现在需要做得就是从句子中随机抽取一个单词，试图通过监督学习网络来预测这个句子中的前 5 个或者后 5 个单词中的一个（网络没有得到其他的上下文环境），也就是该网络的数据就是一个 One-hot 向量，输出层是一个 Softmax 层期望输出另一个 One-hot 向量。
![嵌入矩阵训练方式](../Pic/image-28.png)
训练的时候这个网络只有两个层——输入层、一个*无激活*的隐藏层和 Softmax 输出层，这里使用的就是 Skip-gram 模型，这是 Word2Vec 模型的一种（还有一种比较常用的就是 CBOW, 它和 Skip-gram 的区别其实就在于前者是输入上下文预测一个单词后者是输入一个单词预测上下文，实际做法是相似的，这一点可以在后面的部分参考链接中看到），大致的网络结构是这样的：
![Skip-gram 模型网络结构](../Pic/image-29.png)
它主要有两组权重矩阵$\boldsymbol{W}^{[1]},\boldsymbol{W}^{[2]}$，实际计算的时候就是输入一个 One-hot 向量通过$\boldsymbol{W}^{[1]}$(其实也就是我们需要的权重矩阵$\boldsymbol{E}$)计算出隐藏层神经元：
$$
\boldsymbol{h}=\boldsymbol{W}^{[1]}\boldsymbol{o}_c\\
$$
接下来通过一个共享的权重矩阵$\boldsymbol{W}^{[2]}$计算出 Softmax 层的输出：
$$
\boldsymbol{u}=\boldsymbol{W}^{[2]}\boldsymbol{h}\\
$$
接下来将这个结果进行激活也就是送到一个 Softmax 层中，就可以得到用于计算代价的归一化概率向量：
$$
y_j=\frac{e^{\boldsymbol{u}_j}}{\displaystyle\sum_{i=1}^V e^{\boldsymbol{u}_i}}
$$
然后定义这里的损失函数为：
$$
L(\boldsymbol{\hat y},\boldsymbol{y})=\sum_{i=1}^C y_i\ln\hat y_i
$$
于是就可以通过反向传播梯度下降的方法来优化这两个权重矩阵了，最终得到的第一个权重矩阵就是我们要的嵌入矩阵(上面这一部分内容除了看视频还参考了链接[深入浅出Word2Vec原理解析](https://zhuanlan.zhihu.com/p/114538417)，还有一个有数学计算过程的[【图文并茂】通过实例理解word2vec之Skip-gram](https://zhuanlan.zhihu.com/p/215797088)也讲得不错)。


实际上 Skip-gram 或者说 Word2Vec 最终需要完成的内容并不是实现比较准确的预测词语，而是缕顺某一句话是不是通顺（就像前面的例子，“我生是学”明显是一个不通顺的句子，正常的应该是“我是学生”，单词嵌入就是通过完成这种任务来捕捉不同词语构成句子的规律方法，从而**总结出词性等特征实现降维**）(这一部分可以参看链接[[NLP] 秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)，将得比较清楚)。


不过，由于需要降维的单词数量其实非常多（前面已经说过，这个网络的输出都是 One-hot 向量，因此输入输出维度都和字典中的单词数量相同），因此上面的网络需要进行的运算量是非常大的，为此研究人员提出了加快训练的方法，主要就是 Hierarchical Softmax 和 Negative Sampling.接下来大致介绍一下这两个快速训练的算法。(下面的部分主要还是借鉴了[深入浅出Word2Vec原理解析](https://zhuanlan.zhihu.com/p/114538417)):


### 5.2.1 基于 Hierarchical Softmax 的 Skip-gram
Hierarchical Softmax 其实就是通过数据结构的手段将这样一个多分类问题划分成很多个二分类问题来解决的，通过构建 Hoffman 二叉树将高频出现的词语放在浅层节点、将低频出现的词语放在深层节点，这样就可以在搜索的时候大幅度提高速度(甚至高于完全平衡二叉树)：
![](../Pic/image-30.png)
实际上，该二叉树的每个节点都对应着一个二分类问题，而每个末端节点都对应着一个单词（或者说 One-hot 向量），这样每个单词就对应 Hoffman 二叉树的一条唯一搜索路径，因而也就对应着一个唯一确定的概率（因为一路下来都是二分类，因此这个概率其实已经归一化过了）。比如现在使用$p^w$代表从根节点出发到达单词 w 的路径，$l^w$代表路径$p^w$中包含节点的个数，$p^w_1,p^w_2,\dots,p^w_{l^w}$代表该路径上的各个节点，其中$p^w_1$代表跟节点，$p^w_{l^w}$代表单词 w 所对应的节点，$d_2^w,d_3^w,\dots,d_{l^w}^w\in\{0,1\}$代表每个节点在其父节点的分类结果（左节点 or 右节点）（很显然根节点无法划分该结果所以下标是从 2 开始的），而$\boldsymbol{\theta}_1^w,\boldsymbol{\theta}_2^w,\dots,\boldsymbol{\theta}_{l^w}^w$代表该路径上非叶子节点（就是不处于末端的节点）的参数向量，就是二分类的时候用的向量。于是，如果隐藏层输出的是向量$\boldsymbol{v_w}$的话,对于每个节点来说，进行二分类的时候被分为正类的概率就是
$$
\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^w_i)=\frac{1}{1+e^{-\boldsymbol{v}_w^T\boldsymbol{\theta}^w_i}}\\
$$
于是从根节点出发到达某一个词 u 的概率就是沿路二分类问题分类概率之积：
$$
\begin{align*} p(d_j^u|\boldsymbol{v}(w),\boldsymbol{\theta}_{j-1}^u)&=\begin{cases} \sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\qquad, d_j^u=0,\\ 1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1}) \ ,d_j^u=1. \end{cases}\\ &=\left[\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{1-d_j^u}\cdot\left[1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{d_j^u} \end{align*}\\
$$
<!-- $$
\begin{align*}
p(d_j^u|\boldsymbol{v}(w),\boldsymbol{\theta}_{j-1}^u)&=\begin{cases}
\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\qquad, d_j^u=0,\\
1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1}) \ ,d_j^u=1.
\end{cases}\\
&=\left[\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{1-d_j^u}\cdot\left[1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{d_j^u}
\end{align*}
$$ -->
重新梳理一下现在进行的任务是已知单词 w 计算词库中某个单词 u 出现在其上下文的概率，于是从根节点出发进行二分类到单词最终的概率就是：
$$
p(u|w)=\prod_{j=2}^{l^u}p(d_j^u|\boldsymbol{v}(w),\boldsymbol{\theta}_{j-1}^u)\\
$$
于是可以写出损失函数为：
$$ \begin{align*} L&=-\sum_{w\in C}\ln\prod_{u\in \textrm{Context}(w)}\prod_{j=2}^{l^u}\left[\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{1-d_j^u}\cdot\left[1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{d_j^u}\\ &=-\sum_{w\in C}\sum_{u\in \textrm{Context}(w)}\sum_{j=2}^{l^u}\left[(1-d_j^u)\ln\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})+d_j^u\ln\left(1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right)\right] \end{align*}
$$
<!-- $$
\begin{align*}
L&=-\sum_{w\in C}\ln\prod_{u\in \textrm{Context}(w)}\prod_{j=2}^{l^u}\left[\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{1-d_j^u}\cdot\left[1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right]^{d_j^u}\\
&=-\sum_{w\in C}\sum_{u\in \textrm{Context}(w)}\sum_{j=2}^{l^u}\left[(1-d_j^u)\ln\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})+d_j^u\ln\left(1-\sigma(\boldsymbol{v}_w^T\boldsymbol{\theta}^u_{j-1})\right)\right]
\end{align*}
$$ -->
于是就可以使用梯度下降的方法写出总的代价函数相对于节点中的每个参数以及前级网络输出$\boldsymbol{v}_w$的代价函数了，从而就可以进行梯度下降了，不再赘述。

### 5.2.2 基于 Negative Sampling 的 Skip-gram

负采样方法对于降低计算量的考量就不再是通过强大的数据结构来管理数据了，而是通过随机采样的方法来等效实现。首先可以肯定的是完全随机的采样的最终效果显然是不好的，因为肯定会存在一部分单词(and or the of...)的出现频率远高于其他实词，这里选择的方法就是按照出现频率进行采样（即选择一种随机采样的规则使得出现频率高的单词被随机采样到的可能性更高）。


这里可以展开说说负采样的具体实现，首先需要统计一个文本集合 C 中各个单词的出现频率：
$$
f(w)=\frac{\textrm{Count}(w)}{\displaystyle\sum_{u\in C}\textrm{Count}(u)}\\
$$
这里的$\textrm{Count}(w)$表征的就是单词 w 在语料 C 中的出现频次。在随机的时候可以固定一种词典中所有单词的一种全排列，按照各个单词的出现频次将实数区间[0,1]划分成若干段，每一段都对应一个单词，而每一段的长度都等于该单词的出现频次。在抽样的时候需要随机一个[0,1]之间的实数，找到它对应的是哪一个单词就实现了按照出现频次进行抽样。出现频次较高的单词被抽中的几率更大。


不过实际上算法运行是通过其他手段映射到整数域进行随机的，但是思路是一样的。不过原论文中还有一个细节就是实际上在构造“频次”的时候，使用的是这样的一个式子：
$$
f(w)=\frac{\textrm{Count}(w)^\alpha}{\displaystyle\sum_{u\in C}\textrm{Count}(u)^\alpha}\\
$$
最后选定$\alpha=0.75$，可能是作者尝试了发现这样的效果比较好。


首先需要注意的点就是这里需要完成的任务依然是一个监督学习任务，需要标注的数据标签就是划分两个单词之间是否相聚较近（比如遍历整个文本数据样本，查看 orange 和 juice 是否存在相距小于 5 个单词的情况，如果有就将该输入对应的 target 标记为 1，反之就标记为 0），注意这里的 orange 单词是一个随机给定的单词，而 juice 单词是根据文本中各单词出现频次进行负采样得到的单词，需要构建的数据集样本需要满足正样本（target=1）数目和负样本数目之比为 1/K ，最终可以得到如下的数据集：
![负采样数据集构造](../Pic/image-31.png)
一般来说在数据集比较小的时候将 K 选定为 5-20 比较好，如果数据集够大的话就可以选 2-5 个。有了这样一个数据集就可以构建一个逻辑回归模型，输入变量是 给定词语 c 和 t，输出是他们两个在 K 个单词内出现的概率：
$$
P(y=1|c,t)=\sigma(\boldsymbol{\theta}_t^T\boldsymbol{e}_c)\\
$$
通过这种方式，我们构建的网络的数据集和训练回归需要的计算量就大大减小了，因为原来是需要在任意两个单词之间构建回归的，现在只需要在少数的单词之间进行回归就可以了，有点像协同训练的概念。


还是回到 Skip-gram 任务里面，在给定样本单词 w 和对应的上下文 Context(w) 的情况下，我们写出的最优化目标是（理解上就是需要最大化窗口中上下文单词的出现概率）：
$$
\max\prod_{\widetilde w\in\textrm{Context}(w)}\prod_{u\in(w\cup\textrm{NEU}^{\widetilde{w}}(w))}p(\textrm{Context}|\widetilde{w})\\ \iff\min\sum_{\widetilde w\in\textrm{Context}(w)}\sum_{u\in(w\cup\textrm{NEU}^{\widetilde{w}}(w))}\ln(-p(\textrm{Context}|\widetilde{w}))\\
$$
这里的$\textrm{NEU}^{\widetilde{w}}(w)$是针对单词$\widetilde{w}$生成的负样本子集，其中：
$$
p(u|\widetilde{w})=\begin{cases}     \sigma(\boldsymbol{v}_{\widetilde{w}}^T\boldsymbol{\theta}^u),\qquad L^w(u)=1,\\     1-\sigma(\boldsymbol{v}_{\widetilde{w}}^T\boldsymbol{\theta}^u), \ \ L^w(u)=0. \end{cases}\\
$$
即
$$
p(u|\widetilde{w})=\left[\sigma(\boldsymbol{v}_{\widetilde{w}}^T\boldsymbol{\theta}^u)\right]^{L^w(u)}\cdot\left[1-\sigma(\boldsymbol{v}_{\widetilde{w}}^T\boldsymbol{\theta}^u)\right]^{1-L^w(u)}
$$
这里的$L^w(u)$表示的就是词 u 的标签，因此整体来说最终的代价函数就选取为：
$$
J=\sum_{w\in C}\sum_{\widetilde w\in\textrm{Context}(w)}\sum_{u\in(w\cup\textrm{NEU}^{\widetilde{w}}(w))}\left\{-L^w(u)\ln\sigma(\boldsymbol{v}_{\widetilde{w}}^T\boldsymbol{\theta}^u)-\left[1-L^w(u)\right]\ln\left[1-\sigma(\boldsymbol{v}_{\widetilde{w}}^T\boldsymbol{\theta}^u)\right]\right\}
$$
接下来还是可以使用梯度下降法来训练，不再赘述。
