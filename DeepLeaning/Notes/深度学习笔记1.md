# 深度学习吴恩达老师课堂笔记（一）

主要都是自己上课做的笔记，目录结构参考了[完结撒花！吴恩达DeepLearning.ai《深度学习》课程笔记目录总集_深度学习笔记 吴恩达-CSDN博客](https://blog.csdn.net/koala_tree/article/details/79913655).不过这个博主的笔记也记得挺详细的，这里给点个赞![Alt text](../Pic/doge.png)

## 1. 神经网络和深度学习

【主要都是在讲一些机器学习的内容，所以这里不再重复做笔记了，只记录一些零散的知识点】


ReLU函数(Rectified Linear Unit, 修正线性单元)$\textrm{ReLU}(x)=\max\{x,0\}$，用于替代传统机器学习中的Sigmoid函数，主要用于加快网络的训练速度——sigmoid函数在自变量比较大的时候斜率趋于零因而梯度下降的收敛速度会变慢，而当x大于0的时候ReLU函数的导数一直保持为1，这就大大加快了梯度下降算法的收敛速度。


结构化数据和非结构化数据：

- 一类信息能够用数据或统一的结构加以表示，我们称之为结构化数据，如数字、符号等；
- 而另一类信息无法用数字或统一的结构表示，如文本、图像、声音、网页等，我们称之为非结构化数据。

结构化数据是计算机很容易理解的，相反，计算机对于非结构化数据不知所措；而人类相反地对非结构化数据有很强的适应和解释能力。计算机想要理解非结构化数据还是需要依赖于深度学习网络。


深度学习在最近几年快速发展的原因——人类遇到的数据规模不断提升，传统机器学习在面对超大规模数据时显得捉襟见肘，所以需要更大的网络规模和数据规模来提升网络性能。所以总体来说还是规模带动了深度学习的发展：
![神经网络的性能和数据规模变化情况](../Pic/image1.png)
上图说明在数据量比较小的时候各种算法的优劣性能主要来源于算法设计本身，而当数据规模不断上升的时候大网络展现出的能力逐渐开始优于其他算法。


一般将一个数据样本写为$(\boldsymbol{x},y)$，这里我们将**x**中元素的个数（特征变量数）记作$n_x$，将样本数记作m，将数据样本总体写作矩阵**X**：
$$
\boldsymbol{X}=\begin{bmatrix} \boldsymbol{x}^{(1)} & \boldsymbol{x}^{(2)} & \cdots & \boldsymbol{x}^{(m)} \end{bmatrix}\in\mathbb{R}^{n_x\times m}\\
$$
<!-- $$
\boldsymbol{X}=\begin{bmatrix}
\boldsymbol{x}^{(1)} & \boldsymbol{x}^{(2)} & \cdots & \boldsymbol{x}^{(m)}
\end{bmatrix}\in\mathbb{R}^{n_x\times m}
$$ -->
同时将标签y的集合记作矩阵**Y**：
$$
\boldsymbol{Y}=\begin{bmatrix} y^{(1)} & y^{(2)} & \cdots & y^{(m)} \end{bmatrix}\in\mathbb{R}^{1\times m}
$$
<!-- $$
\boldsymbol{Y}=\begin{bmatrix}
y^{(1)} & y^{(2)} & \cdots & y^{(m)}
\end{bmatrix}\in\mathbb{R}^{1\times m}
$$ -->
在逻辑回归中，想要完成二分类任务，需要构建参数$\boldsymbol{\omega}\in\mathbb{R}^{n_x},b\in\mathbb{R}$使得输出为：
$$
\hat y=\textrm{Sigmoid}(\boldsymbol{\omega}^T\boldsymbol{x}+b)=P(y=1|\boldsymbol{x})\text{ where }\textrm{Sigmoid}(z)=\frac{1}{1+e^{-z}}\\
$$
这里变量z和之前的定义一样：
$$
z=\boldsymbol{\omega}^T\boldsymbol{x}+b\\
$$
【注意这里的符号习惯于机器学习不一样，机器学习习惯采用整个的$\boldsymbol{\theta}$同时表示偏置和特征变量增益，而这里将$\boldsymbol{\omega}$与$b$分开分别看成两个变量】


为了保证损失函数是凸函数，这里定义的损失函数并不是常规在线性回归中使用的误差平方模型，而是对数模型（其实这就是从二项分布的极大似然函数里面推导出来的$P(y|\boldsymbol{x})=\hat y^y(1-\hat y)^{1-y}$，我们希望这个似然函数越大越好，因此将损失函数取为它的负对数就是下面这个结果）：
$$
L(\hat y,y)=-y\ln\hat y-(1-y)\ln(1-\hat y)\\
$$
而整个模型的**代价函数**就表述为各个样本**损失函数**之和：
$$
J(\boldsymbol{\omega},b)=\frac{1}{m}\sum_{i=1}^m L\left(\hat y^{(i)},y^{(i)}\right)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\ln\hat y^{(i)}+(1-y^{(i)})\ln(1-\hat y^{(i)})\right]\\
$$
在写代码的时候一般会规定将$\displaystyle\frac{\textrm{d}J}{\textrm{d}var}$命名为“dvar”。


值得注意的就是大多数现成的运算库能实现比较高速度的向量运算，下面这段代码的最终运行结果表明，使用numpy自带的向量点乘相比直接调用python的for循环进行向量点乘在速度上有数量级的提升：
```python
import time
import numpy as np
costTim=[[],[]]
dataLen=10000000
maxRange=45

for i in range(0,maxRange):
    a=np.random.rand(dataLen,1)
    b=np.random.rand(dataLen,1)

    tic=time.time()
    c=np.dot(a.T,b)
    toc=time.time()

    costTim[0].append((toc-tic)*1000)

    c=0
    a=a.tolist()
    b=b.tolist()
    tic=time.time()
    for j in range(dataLen):
        c+=a[j][0]*b[j][0]
    toc=time.time()

    costTim[1].append((toc-tic)*1000)

    print(str((i+1)/maxRange*100)+"%")

for index in [0,1]:
    if index == 0:
        print("Vectorized version:")
    elif index == 1:
        print("For loop version:")
    meanTim = np.mean(costTim[index])
    maxTim = np.max(costTim[index])
    minTim = np.min(costTim[index])
    print("average calc time:\t"+str(meanTim)+"ms")
    print("max calc time:\t\t"+str(maxTim)+"ms")
    print("min calc time:\t\t"+str(minTim)+"ms\n")
```
![程序运行结果](../Pic/image2.png)
这给我们的启示就是在编写新的算法的程序的时候可以用向量计算代替for循环的地方就尽可能使用向量计算，这也是为什么在机器学习中大力推崇运算的向量化的原因。


使用向量化的思路，正向传播的程序就是两行：
```python
Z = np.dot(w.T, X) + b
A = sigmoid(Z)
```
同样的，梯度计算也是可以向量化实现的：
```python
dZ = A - Y
db = np.sum(dZ) / m
dX = np.dot(X, dZ.T) / m
```
反向传播更是不同再赘述了。


这里值得一提的就是python中的广播机制，这里的广播机制描述的就是numpy可以对数学上不完全能运算的矩阵进行自动联想运算。比如现在有一个m×n的矩阵想要和1×n（也可以是m×1）的矩阵进行相加/减/乘（按元素）/除（按元素），numpy会自动将维数不够的矩阵进行复制填充（在这个例子里面就是对后者复制m行）以满足正常运算，这就是广播机制。


还有就是numpy中的一些神奇操作，比如在使用random创建数组的时候，如果使用`np.random.rand(N)`语句得到的实际上是一个“一秩数组”，有些情况下它的行为和行向量、列向量并不一致（比如获得大小、类型、向量运算等），所以建议在创建的时候使用`np.random.rand(N,1)`这种写法，可以避免bug的出现。如果不知道定义的数据类型，可以在程序中使用断言`assert(a.shape == (N,1))`，当然必要的时候也可以使用`a=a.reshape(N,1)`进行转换。


在神经网络参数描述中，深度学习习惯使用方括号来说明参数所在网络层数，比如$\boldsymbol{W}^{[1]}$就说明这是第一层神经网络的参数矩阵：
![网络参数的定义方式（和机器学习稍有出入）](../Pic/image3.png)
习惯上来说，我们会将上面的这个神经网络称为两层的神经网络（一般不把输入层看作标准的一层网络），但是每个神经元内进行的运算实际上是和逻辑回归是完全一致的：
